# Input data
data_url: ""
load_checkpoint_url: ""
vocab_file: ""

#output data
output_url: ""

#NPU config
device_target: "Ascend"
enable_profiling: False
load_pretrain: True
use_pynative_mode: False
device_id: 0
enable_modelarts: False

#task config
do_train: False
do_eval: False
do_predict: False
metrics: ACC
with_lstm: False

#loss config, if use Focal Loss, set loss_name: Focal and set loss_gamma and label_percent
loss_name: CrossEntropy
label_percent: "1:1"
loss_gamma: 2.0

#train_config
epoch_num: 20
frozen_bert: False
task_name: "Task"
cut_layer_num: 0

#predict config
return_sequence: False
return_csv: False
print_predict: False

#classification/sequence/correlation global config
num_class: 2
description: ""

#data config
train_data_shuffle: "true"
eval_data_shuffle: "false"
train_batch_size: 32
eval_batch_size: 1
dataset_format: "mindrecord"
file_format: 'MINDIR'

#network config
optimizer_cfg:
    optimizer: 'AdamWeightDecay'
    AdamWeightDecay:
        learning_rate: 0.00001  # 2e-5
        end_learning_rate: 0.0000000001  # 1e-10
        power: 1.0
        weight_decay: 0.00001  # 1e-5
        decay_filter: ['layernorm', 'bias']
        eps: 0.000001  # 1e-6
    Lamb:
        learning_rate: 0.00002  # 2e-5,
        end_learning_rate: 0.0000000001  # 1e-10
        power: 1.0
        weight_decay: 0.01
        decay_filter: ['layernorm', 'bias']
    Momentum:
        learning_rate: 0.00002  # 2e-5
        momentum: 0.9

bert_net_cfg:
    seq_length: 2048
    vocab_size: 25
    hidden_size: 512
    num_hidden_layers: 6
    num_attention_heads: 4
    intermediate_size: 3072
    hidden_act: "gelu"
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1
    max_position_embeddings: 2048
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: False
    dtype: mstype.float32
    compute_type: mstype.float16

---
# chocies
description: ["classification","correlation","sequence","regress"]
